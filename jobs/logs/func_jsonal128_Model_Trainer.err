12/10/2022 22:16:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/10/2022 22:16:55 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x2b24b53fd3d0>
12/10/2022 22:17:31 - INFO - __main__ -   *** Example ***
12/10/2022 22:17:31 - INFO - __main__ -   idx: 0
12/10/2022 22:17:31 - INFO - __main__ -   label: 0
12/10/2022 22:17:31 - INFO - __main__ -   input_tokens: ['<s>', 'int', '_apply', '_', 'filter', '_', 'to', '_', 'req', '_', 'headers', '(', 'struct', '_session', '_*', 's', ',', '_struct', '_channel', '_*', 'req', ',', '_struct', '_h', 'dr', '_', 'exp', '_*', 'exp', ')', '_{', '_char', '_*', 'cur', '_', 'ptr', ',', '_*', 'cur', '_', 'end', ',', '_*', 'cur', '_', 'next', ';', '_int', '_cur', '_', 'id', 'x', ',', '_old', '_', 'id', 'x', ',', '_last', '_', 'h', 'dr', ';', '_struct', '_http', '_', 'tx', 'n', '_*', 'tx', 'n', '_=', '_&', 's', '->', 'tx', 'n', ';', '_struct', '_h', 'dr', '_', 'id', 'x', '_', 'e', 'lem', '_*', 'cur', '_', 'h', 'dr', ';', '_int', '_delta', ';', '_last', '_', 'h', 'dr', '_=', '_0', ';', '_cur', '_', 'next', '_=', '_req', '->', 'buf', '->', 'p', '_+', '_h', 'dr', '_', 'id', 'x', '_', 'first', '_', 'pos', '(&', 'tx', 'n', '->', 'h', 'dr', '_', 'id', 'x', ');', '_old', '_', 'id', 'x', '_=', '_0', ';', '_while', '_(!', 'last', '_', 'h', 'dr', ')', '_{', '_if', '_(', 'un', 'likely', '(', 'tx', 'n', '->', 'flags', '_&', '_(', 'TX', '_', 'CL', 'DEN', 'Y', '_|', '_TX', '_', 'CL', 'T', 'ARP', 'IT', ')))', '_return', '_1', ';', '_else', '_if', '_(', 'un', 'likely', '(', 'tx', 'n', '->', 'flags', '_&', '_TX', '_', 'CL', 'ALL', 'OW', ')', '_&&', '_(', 'exp', '->', 'action', '_==', '_ACT', '_', 'ALL', 'OW', '_||', '_exp', '->', 'action', '_==', '_ACT', '_', 'DEN', 'Y', '_||', '_exp', '->', 'action', '_==', '_ACT', '_', 'T', 'ARP', 'IT', '))', '_return', '_0', ';', '_cur', '_', 'id', 'x', '_=', '_tx', 'n', '->', 'h', 'dr', '_', 'id', 'x', '.', 'v', '[', 'old', '_', 'id', 'x', '].', 'next', ';', '_if', '_(!', 'cur', '_', 'id', 'x', ')', '_break', ';', '_cur', '_', 'h', 'dr', '_=', '_&', 'tx', 'n', '->', 'h', 'dr', '_', 'id', 'x', '.', 'v', '[', 'cur', '_', 'id', 'x', '];', '_cur', '_', 'ptr', '_=', '_cur', '_', 'next', ';', '_cur', '_', 'end', '_=', '_cur', '_', 'ptr', '_+', '_cur', '_', 'h', 'dr', '->', 'len', ';', '_cur', '_', 'next', '_=', '_cur', '_', 'end', '_+', '_cur', '_', 'h', 'dr', '->', 'cr', '_+', '_1', ';', '_/*', '_Now', '_we', '_have', '_one', '_header', '_between', '_cur', '_', 'ptr', '_and', '_cur', '_', 'end', ',', '_*', '_and', '_the', '_next', '_header', '_starts', '_at', '_cur', '_', 'next', '.', '_*/', '_if', '_(', 're', 'gex', '_', 'exec', '_', 'match', '2', '(', 'exp', '->', 'p', 'reg', ',', '_cur', '_', 'ptr', ',', '_cur', '_', 'end', '-', 'cur', '_', 'ptr', ',', '_MAX', '_', 'M', 'ATCH', ',', '_pm', 'atch', '))', '_{', '_switch', '_(', 'exp', '->', 'action', ')', '_{', '_case', '_ACT', '_', 'SET', 'BE', ':', '_/*', '_It', '_is', '_not', '_possible', '</s>']
12/10/2022 22:17:31 - INFO - __main__ -   input_ids: 0 2544 3253 1215 46617 1215 560 1215 47278 1215 47288 1640 25384 1852 1009 29 6 29916 4238 1009 47278 6 29916 1368 10232 1215 18793 1009 18793 43 25522 16224 1009 17742 1215 43880 6 1009 17742 1215 1397 6 1009 17742 1215 25616 131 6979 5350 1215 808 1178 6 793 1215 808 1178 6 94 1215 298 10232 131 29916 2054 1215 43820 282 1009 43820 282 5457 359 29 46613 43820 282 131 29916 1368 10232 1215 808 1178 1215 242 10770 1009 17742 1215 298 10232 131 6979 38744 131 94 1215 298 10232 5457 321 131 5350 1215 25616 5457 48829 46613 48939 46613 642 2055 1368 10232 1215 808 1178 1215 9502 1215 11474 49763 43820 282 46613 298 10232 1215 808 1178 4397 793 1215 808 1178 5457 321 131 150 48209 13751 1215 298 10232 43 25522 114 36 879 30266 1640 43820 282 46613 46760 359 36 30200 1215 7454 28082 975 1721 12031 1215 7454 565 30711 2068 47619 671 112 131 1493 114 36 879 30266 1640 43820 282 46613 46760 359 12031 1215 7454 7981 4581 43 48200 36 18793 46613 10845 45994 12928 1215 7981 4581 45056 4553 46613 10845 45994 12928 1215 28082 975 45056 4553 46613 10845 45994 12928 1215 565 30711 2068 35122 671 321 131 5350 1215 808 1178 5457 48726 282 46613 298 10232 1215 808 1178 4 705 10975 279 1215 808 1178 8174 25616 131 114 48209 17742 1215 808 1178 43 1108 131 5350 1215 298 10232 5457 359 43820 282 46613 298 10232 1215 808 1178 4 705 10975 17742 1215 808 1178 44082 5350 1215 43880 5457 5350 1215 25616 131 5350 1215 1397 5457 5350 1215 43880 2055 5350 1215 298 10232 46613 8476 131 5350 1215 25616 5457 5350 1215 1397 2055 5350 1215 298 10232 46613 8344 2055 112 131 48565 978 52 33 65 12734 227 5350 1215 43880 8 5350 1215 1397 6 1009 8 5 220 12734 2012 23 5350 1215 25616 4 48404 114 36 241 45767 1215 22342 1215 10565 176 1640 18793 46613 642 4950 6 5350 1215 43880 6 5350 1215 1397 12 17742 1215 43880 6 28066 1215 448 24321 6 4751 11175 35122 25522 5405 36 18793 46613 10845 43 25522 403 12928 1215 45099 8827 35 48565 85 16 45 678 2
12/10/2022 22:17:31 - INFO - __main__ -   *** Example ***
12/10/2022 22:17:31 - INFO - __main__ -   idx: 1
12/10/2022 22:17:31 - INFO - __main__ -   label: 1
12/10/2022 22:17:31 - INFO - __main__ -   input_tokens: ['<s>', 'String', '_Text', 'Cod', 'ec', 'UTF', '8', '::', 'Dec', 'ode', '(', 'const', '_char', '*', '_bytes', ',', '_w', 'tf', '_', 'size', '_', 't', '_length', ',', '_Fl', 'ush', 'Beh', 'avior', '_flush', ',', '_bool', '_stop', '_', 'on', '_', 'error', ',', '_bool', '&', '_saw', '_', 'error', ')', '_{', '_const', '_bool', '_do', '_', 'flush', '_=', '_flush', '_!=', '_Fl', 'ush', 'Beh', 'avior', '::', 'k', 'Do', 'Not', 'Fl', 'ush', ';', '_String', 'Buffer', '<', 'L', 'Char', '>', '_buffer', '(', 'partial', '_', 'sequence', '_', 'size', '_', '_+', '_length', ');', '_const', '_uint', '8', '_', 't', '*', '_source', '_=', '_re', 'interpret', '_', 'cast', '<', 'const', '_uint', '8', '_', 't', '*', '>(', 'bytes', ');', '_const', '_uint', '8', '_', 't', '*', '_end', '_=', '_source', '_+', '_length', ';', '_const', '_uint', '8', '_', 't', '*', '_aligned', '_', 'end', '_=', '_Al', 'ign', 'To', 'Machine', 'Word', '(', 'end', ');', '_L', 'Char', '*', '_destination', '_=', '_buffer', '.', 'Characters', '();', '_do', '_{', '_if', '_(', 'partial', '_', 'sequence', '_', 'size', '_', ')', '_{', '_L', 'Char', '*', '_destination', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', '_=', '_destination', ';', '_const', '_uint', '8', '_', 't', '*', '_source', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', '_=', '_source', ';', '_if', '_(', 'Handle', 'Part', 'ial', 'Sequ', 'ence', '(', 'dest', 'ination', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', ',', '_source', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', ',', '_end', ',', '_do', '_', 'flush', ',', '_stop', '_', 'on', '_', 'error', ',', '_saw', '_', 'error', '))', '_{', '_source', '_=', '_source', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', ';', '_goto', '_up', 'Con', 'vert', 'To', '16', 'Bit', ';', '_}', '_destination', '_=', '_destination', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', ';', '_source', '_=', '_source', '_', 'for', '_', 'handle', '_', 'partial', '_', 'sequence', ';', '_if', '_(', 'partial', '_', 'sequence', '_', 'size', '_', ')', '_break', ';', '_}', '_while', '_(', 'source', '_<', '_end', ')', '_{', '_if', '_(', 'Is', 'ASC', 'II', '(*', 'source', '))', '_{', '_if', '_(', 'Is', 'Al', 'igned', 'To', 'Machine', 'Word', '(', 'source', '))', '_{', '_while', '_(', 'source', '_<', '_aligned', '_', 'end', ')', '_{', '_Machine', 'Word', '_chunk', '_=', '_*', 're', 'interpret', '_', 'cast', '_', 'ptr', '<', 'const', '_Machine', 'Word', '*', '>(', 'source', ');', '_if', '_(!', 'Is', 'All', 'ASC', 'II', '<', 'L', 'Char', '>(', 'ch', 'unk', '))', '_break', ';', '_Copy', 'AS', 'CI', 'IM', 'achine', 'Word', '(', 'dest', 'ination', ',', '_source', ');', '_source', '_+=', '_sizeof', '(', 'Machine', 'Word', ');', '_destination', '_+=', '_sizeof', '(', 'Machine', 'Word', ');', '_}', '_if', '_(', 'source', '_==', '_end', ')', '_break', ';', '_if', '_(!', 'Is', 'ASC', '</s>']
12/10/2022 22:17:31 - INFO - __main__ -   input_ids: 0 34222 14159 47436 3204 44987 398 38304 15953 4636 1640 20836 16224 3226 46487 6 885 41407 1215 10799 1215 90 5933 6 4150 3810 47001 47475 24841 6 49460 912 1215 261 1215 44223 6 49460 947 794 1215 44223 43 25522 10759 49460 109 1215 47742 5457 24841 49333 4150 3810 47001 47475 38304 330 8275 7199 16197 3810 131 26602 49334 41552 574 42379 15698 21944 1640 45593 1215 46665 1215 10799 1215 2055 5933 4397 10759 49315 398 1215 90 3226 1300 5457 769 41111 1215 5182 41552 20836 49315 398 1215 90 3226 49925 46823 4397 10759 49315 398 1215 90 3226 253 5457 1300 2055 5933 131 10759 49315 398 1215 90 3226 14485 1215 1397 5457 726 4932 3972 46100 44051 1640 1397 4397 226 42379 3226 6381 5457 21944 4 49282 47006 109 25522 114 36 45593 1215 46665 1215 10799 1215 43 25522 226 42379 3226 6381 1215 1990 1215 26628 1215 45593 1215 46665 5457 6381 131 10759 49315 398 1215 90 3226 1300 1215 1990 1215 26628 1215 45593 1215 46665 5457 1300 131 114 36 48678 4741 2617 48245 4086 1640 31549 8111 1215 1990 1215 26628 1215 45593 1215 46665 6 1300 1215 1990 1215 26628 1215 45593 1215 46665 6 253 6 109 1215 47742 6 912 1215 261 1215 44223 6 794 1215 44223 35122 25522 1300 5457 1300 1215 1990 1215 26628 1215 45593 1215 46665 131 49325 62 9157 9942 3972 1549 36918 131 35524 6381 5457 6381 1215 1990 1215 26628 1215 45593 1215 46665 131 1300 5457 1300 1215 1990 1215 26628 1215 45593 1215 46665 131 114 36 45593 1215 46665 1215 10799 1215 43 1108 131 35524 150 36 17747 28696 253 43 25522 114 36 6209 40230 11194 49570 17747 35122 25522 114 36 6209 7083 9044 3972 46100 44051 1640 17747 35122 25522 150 36 17747 28696 14485 1215 1397 43 25522 14969 44051 15836 5457 1009 241 41111 1215 5182 1215 43880 41552 20836 14969 44051 3226 49925 17747 4397 114 48209 6209 3684 40230 11194 41552 574 42379 49925 611 6435 35122 1108 131 22279 2336 21701 3755 47480 44051 1640 31549 8111 6 1300 4397 1300 49371 49907 1640 46100 44051 4397 6381 49371 49907 1640 46100 44051 4397 35524 114 36 17747 45994 253 43 1108 131 114 48209 6209 40230 2
12/10/2022 22:17:31 - INFO - __main__ -   *** Example ***
12/10/2022 22:17:31 - INFO - __main__ -   idx: 2
12/10/2022 22:17:31 - INFO - __main__ -   label: 0
12/10/2022 22:17:31 - INFO - __main__ -   input_tokens: ['<s>', 'static', '_const', '_char', '_*', 'Windows', 'Error', 'Str', '(', 'DW', 'ORD', '_dw', 'Message', 'Id', ')', '_{', '_static', '_LP', 'STR', '_msg', '_=', '_NULL', ';', '_if', '_(', 'msg', ')', '_Local', 'Free', '(', 'msg', ');', '_if', '_(', 'Format', 'Message', '(', 'FORM', 'AT', '_', 'M', 'ES', 'SA', 'GE', '_', 'ALL', 'OC', 'ATE', '_', 'BU', 'FFER', '_|', '_FOR', 'MAT', '_', 'M', 'ES', 'SA', 'GE', '_', 'FR', 'OM', '_', 'SY', 'STEM', ',', '_0', ',', '_dw', 'Message', 'Id', ',', '_0', ',', '_(', 'LP', 'STR', ')', '&', 'msg', ',', '_0', ',', '_0', '))', '_return', '_msg', ';', '_static', '_const', '_char', '_fmt', '[]', '_=', '_"', 'Error', '_#', '%', 'ld', '";', '_signed', '_long', '_l', 'd', 'Msg', 'Id', '_=', '_dw', 'Message', 'Id', ';', '_int', '_s', 'z', '_=', '_sn', 'printf', '((', 'char', '*)', '&', 's', 'z', ',', '_0', ',', '_fmt', ',', '_l', 'd', 'Msg', 'Id', ')', '_+', '_1', ';', '_msg', '_=', '_(', 'L', 'PT', 'STR', ')', 'Local', 'All', 'oc', '(', 'LM', 'EM', '_', 'FIX', 'ED', ',', '_s', 'z', ');', '_sprint', 'f', '((', 'char', '*)', 'msg', ',', '_fmt', ',', '_l', 'd', 'Msg', 'Id', ');', '_return', '_msg', ';', '_}', '</s>']
12/10/2022 22:17:31 - INFO - __main__ -   input_ids: 0 42653 10759 16224 1009 43815 30192 29116 1640 39751 11200 42538 42394 28081 43 25522 25156 8765 30549 49049 5457 48955 131 114 36 48593 43 4004 18074 1640 48593 4397 114 36 48587 42394 1640 38036 2571 1215 448 1723 3603 8800 1215 7981 4571 8625 1215 19159 45234 1721 5089 41229 1215 448 1723 3603 8800 1215 5499 3765 1215 21134 43896 6 321 6 42538 42394 28081 6 321 6 36 21992 30549 43 947 48593 6 321 6 321 35122 671 49049 131 25156 10759 16224 49843 48992 5457 22 30192 849 207 4779 25718 1419 251 784 417 49680 28081 5457 42538 42394 28081 131 6979 579 329 5457 4543 49775 48461 24262 44431 947 29 329 6 321 6 49843 6 784 417 49680 28081 43 2055 112 131 49049 5457 36 574 10311 30549 43 24476 3684 1975 1640 21672 5330 1215 47787 1691 6 579 329 4397 12631 506 48461 24262 44431 48593 6 49843 6 784 417 49680 28081 4397 671 49049 131 35524 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
12/10/2022 22:17:37 - INFO - __main__ -   ***** Running training *****
12/10/2022 22:17:37 - INFO - __main__ -     Num examples = 13796
12/10/2022 22:17:37 - INFO - __main__ -     Num Epochs = 5
12/10/2022 22:17:37 - INFO - __main__ -     Instantaneous batch size per GPU = 128
12/10/2022 22:17:37 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 128
12/10/2022 22:17:37 - INFO - __main__ -     Gradient Accumulation steps = 1
12/10/2022 22:17:37 - INFO - __main__ -     Total optimization steps = 540
  0%|          | 0/108 [00:00<?, ?it/s]  0%|          | 0/108 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "run.py", line 549, in <module>
    main(arg)
  File "run.py", line 440, in main
    train(args, train_dataset, model, tokenizer)
  File "run.py", line 195, in train
    loss,logits = model(inputs,labels)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/TransformerExplainability/code/model.py", line 24, in forward
    outputs=self.encoder(input_ids, attention_mask=input_ids.ne(1))
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 1224, in forward
    return_dict=return_dict,
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 863, in forward
    return_dict=return_dict,
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 534, in forward
    output_attentions,
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 417, in forward
    past_key_value=self_attn_past_key_value,
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 346, in forward
    output_attentions,
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/LAS/weile-lab/mdrahman/SliceLevelVulnerabilityDetection/env/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py", line 235, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 938.00 MiB (GPU 0; 39.41 GiB total capacity; 37.36 GiB already allocated; 542.50 MiB free; 37.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
