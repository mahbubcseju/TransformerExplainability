{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import json\n",
    "# try:\n",
    "#     from torch.utils.tensorboard import SummaryWriter\n",
    "# except:\n",
    "#     from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from model import Model\n",
    "cpu_cont = multiprocessing.cpu_count()\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "Second  torch.Size([1, 400, 400])\n",
      "torch.Size([1, 1, 400])\n",
      "States:  torch.Size([1, 1, 400])\n",
      "Start total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "step gradients size :  torch.Size([1, 12, 400, 400])\n",
      "Step total gradients  torch.Size([1, 12, 400, 400])\n",
      "torch.Size([1, 400])\n",
      "tensor([[1.0339e-02, 1.1610e-04, 6.4848e-05, 1.7424e-04, 8.2446e-04, 2.8418e-04,\n",
      "         1.6117e-04, 7.8237e-05, 4.1007e-05, 1.6176e-04, 1.2027e-04, 7.7694e-05,\n",
      "         1.0504e-05, 1.4855e-06, 6.7374e-06, 5.6062e-05, 1.3936e-05, 1.1771e-04,\n",
      "         3.9610e-05, 5.0144e-05, 6.0669e-05, 6.1955e-05, 1.1975e-04, 5.7399e-05,\n",
      "         3.2704e-04, 3.2591e-04, 1.4821e-04, 2.7704e-04, 4.2039e-04, 5.3937e-04,\n",
      "         5.3781e-04, 4.0232e-04, 7.8912e-05, 1.3202e-05, 1.3419e-05, 1.9082e-05,\n",
      "         8.4006e-06, 1.5487e-05, 7.2208e-06, 7.2446e-05, 5.9772e-04, 1.8737e-04,\n",
      "         4.2672e-04, 1.4424e-04, 3.4613e-05, 3.2716e-06, 5.6560e-06, 1.2373e-05,\n",
      "         2.9386e-06, 7.7681e-06, 3.1818e-06, 1.4609e-04, 2.1500e-04, 8.2983e-04,\n",
      "         2.1970e-03, 1.0060e-03, 3.9547e-04, 3.1078e-04, 3.2753e-04, 4.7397e-04,\n",
      "         5.3044e-04, 1.3482e-04, 1.1984e-04, 5.9990e-05, 3.7655e-05, 7.7344e-05,\n",
      "         5.8925e-04, 5.7760e-04, 4.0194e-04, 4.9400e-04, 7.5314e-05, 1.3997e-04,\n",
      "         9.8912e-04, 1.7170e-03, 1.8024e-03, 1.5427e-03, 6.4437e-03, 3.7764e-03,\n",
      "         2.2173e-03, 1.0791e-03, 2.0763e-04, 1.4665e-04, 8.9866e-05, 7.9614e-05,\n",
      "         8.3870e-05, 5.4438e-04, 1.1437e-04, 2.1175e-04, 3.8530e-04, 2.8895e-04,\n",
      "         6.8165e-04, 5.3548e-04, 1.2283e-04, 9.9732e-06, 4.4202e-05, 2.3273e-05,\n",
      "         3.4139e-05, 3.0252e-05, 2.8774e-05, 6.2099e-04, 1.5304e-04, 6.2742e-04,\n",
      "         1.6226e-04, 1.2110e-04, 6.3061e-05, 6.3905e-05, 7.5575e-05, 3.7613e-04,\n",
      "         9.9191e-05, 2.8126e-04, 2.7647e-04, 2.3061e-04, 4.1935e-04, 5.2077e-04,\n",
      "         3.5374e-04, 0.0000e+00, 2.8632e-05, 2.1155e-05, 2.6924e-05, 2.6729e-05,\n",
      "         2.3640e-05, 6.6464e-07, 3.6973e-04, 1.3947e-04, 4.3579e-04, 1.2568e-04,\n",
      "         9.3222e-05, 5.0457e-05, 5.4673e-05, 6.6496e-05, 2.7827e-04, 9.1824e-05,\n",
      "         2.2802e-04, 2.0934e-04, 1.5660e-04, 3.0042e-04, 8.1295e-04, 3.3939e-04,\n",
      "         7.8382e-05, 1.1505e-04, 3.6298e-05, 5.3993e-05, 5.2963e-05, 5.7968e-05,\n",
      "         3.8843e-05, 2.5324e-04, 1.5969e-04, 3.5510e-04, 1.7205e-04, 1.0729e-04,\n",
      "         7.2434e-05, 7.3282e-05, 9.6425e-05, 3.5488e-04, 1.6429e-04, 3.1086e-04,\n",
      "         2.6470e-04, 1.9338e-04, 3.6231e-04, 7.7444e-04, 1.9713e-04, 5.9962e-05,\n",
      "         1.2397e-04, 1.1231e-04, 1.2362e-04, 3.4430e-04, 2.2169e-04, 4.3540e-04,\n",
      "         2.4957e-04, 1.6095e-04, 1.4283e-04, 1.3165e-04, 1.6006e-04, 5.9566e-04,\n",
      "         2.2263e-04, 4.0231e-04, 3.9704e-04, 2.3892e-04, 6.2165e-04, 1.1687e-03,\n",
      "         5.0665e-04, 8.8113e-05, 1.5029e-04, 2.8293e-04, 2.3666e-04, 6.7750e-04,\n",
      "         3.2816e-04, 1.0355e-03, 2.3379e-03, 1.2786e-03, 9.8456e-04, 1.2165e-03,\n",
      "         1.6298e-03, 1.6524e-03, 2.1332e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "class Model(nn.Module):   \n",
    "    def __init__(self, encoder,config,tokenizer):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.config=config\n",
    "        self.tokenizer=tokenizer\n",
    "        # self.args=args\n",
    "        # self.softmax = torch.softmax(dim=-1)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        \n",
    "    def forward(self, input_ids, inputs_embeds=None,labels=None): \n",
    "        outputs=self.encoder(inputs_embeds=inputs_embeds, attention_mask=input_ids.ne(1))\n",
    "        logits=outputs[0]\n",
    "        prob=torch.softmax(logits, dim=-1)\n",
    "        if labels is not None:\n",
    "            print(prob.shape, labels.shape)\n",
    "            return self.criterion(prob, labels), prob, outputs[1], outputs[2]\n",
    "        else:\n",
    "            return prob, outputs[1], outputs[2]\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_tokens,\n",
    "                 input_ids,\n",
    "                 idx,\n",
    "                 label,\n",
    "\n",
    "    ):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.idx=str(idx)\n",
    "        self.label=label\n",
    "\n",
    "def convert_examples_to_features(js,tokenizer,args):\n",
    "    #source\n",
    "    code=' '.join(js['func'].split())\n",
    "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
    "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "    padding_length = args.block_size - len(source_ids)\n",
    "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    return InputFeatures(source_tokens,source_ids,js[args.idx_key],js['target'])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, file_path=None):\n",
    "        self.examples = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                js=json.loads(line.strip())\n",
    "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        labels = np.zeros((2,))\n",
    "        labels[self.examples[i].label] = 1\n",
    "        return torch.tensor(self.examples[i].input_ids), torch.tensor(self.examples[i].label)\n",
    "\n",
    "\n",
    "class CodebertModel:\n",
    "    \"\"\"Simple sentiment analysis model.\"\"\"\n",
    "\n",
    "    LABELS = [0, 1]  # negative, positive\n",
    "    compute_grads: bool = True  # if True, compute and return gradients.\n",
    "    config_class, model_class, tokenizer_class = RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path = 'microsoft/codebert-base'\n",
    "        self.model_config = self.config_class.from_pretrained(self.model_name_or_path,\n",
    "                num_labels=2,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=True)\n",
    "        self.tokenizer = self.tokenizer_class.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "        model = self.model_class.from_pretrained(self.model_name_or_path,\n",
    "                                                from_tf=bool('.ckpt' in self.model_name_or_path),\n",
    "                                                 config=self.model_config)\n",
    "        self.model = Model(model, self.model_config,  self.tokenizer)\n",
    "        self.output_dir=\"./saved_models\"\n",
    "\n",
    "    def activate_evaluation(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, args):\n",
    "        checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
    "        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))\n",
    "        self.model.load_state_dict(torch.load(output_dir, map_location=args.device))\n",
    "        self.model.to(args.device)\n",
    "\n",
    "\n",
    "    def test(self, args):\n",
    "        model = self.model\n",
    "        tokenizer = self.tokenizer\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n",
    "\n",
    "\n",
    "        # args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        # multi-gpu evaluate\n",
    "        # if args.n_gpu > 1:\n",
    "        #     model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running Test *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        model.eval()\n",
    "        logits=[]   \n",
    "        labels=[]\n",
    "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
    "            inputs = batch[0].to(args.device)        \n",
    "            label=batch[1].to(args.device) \n",
    "            with torch.no_grad():\n",
    "                logit = model(inputs)\n",
    "                logits.append(logit.cpu().numpy())\n",
    "                labels.append(label.cpu().numpy())\n",
    "\n",
    "        logits=np.concatenate(logits,0)\n",
    "        labels=np.concatenate(labels,0)\n",
    "\n",
    "        acts = np.array([0 if (sample[0] > sample[1]) else 1 for sample in labels])\n",
    "        preds = np.array([0 if (sample[0] > sample[1]) else 1 for sample in logits])\n",
    "        \n",
    "        eval_acc=np.mean(acts==preds)\n",
    "        print(eval_acc)\n",
    "    \n",
    "\n",
    "    def explain(self, input_ids, labels, steps=20, start_layer=4):\n",
    "        self.activate_evaluation()\n",
    "        input = self.model.encoder.roberta.embeddings(input_ids)\n",
    "        b = input.shape[0]\n",
    "\n",
    "        output, _, attention = self.model(input_ids, inputs_embeds=input)\n",
    "\n",
    "        b, h, s, _ = attention[-1].shape\n",
    "        num_blocks = len(attention)\n",
    "\n",
    "        states = attention[-1].mean(1)[:, 0, :].reshape(b, 1, s)\n",
    "        for i in range(start_layer, num_blocks - 1)[::-1]:\n",
    "            attn = attention[i].mean(1)\n",
    "            states_ = states\n",
    "            states = states.bmm(attn)\n",
    "            states += states_\n",
    "\n",
    "        total_gradients = torch.zeros(b, h, s, s).cpu()\n",
    "        for alpha in np.linspace(0, 1, steps):        \n",
    "            # forward propagation\n",
    "            data_scaled = input * alpha\n",
    "            # backward propagation\n",
    "            output, _, attention = self.model(input_ids, data_scaled)\n",
    "            one_hot = np.zeros((b, 2), dtype=np.float32)\n",
    "            one_hot[np.arange(b), labels] = 1\n",
    "            one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "            one_hot = torch.sum(one_hot.cpu() * output)\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            attention[-1].retain_grad()\n",
    "            one_hot.backward(retain_graph=True)\n",
    "\n",
    "            # cal grad\n",
    "            gradients = attention[-1].grad\n",
    "            total_gradients += gradients\n",
    "\n",
    "        W_state = (total_gradients / steps).clamp(min=0).mean(1)[:, 0, :].reshape(b, 1, s)\n",
    "        states = states * W_state\n",
    "        return states[:, 0, :]\n",
    "\n",
    "    def explanation(self, args):\n",
    "        self.weights = self.model.encoder.get_input_embeddings()\n",
    "        model = self.model\n",
    "        tokenizer = self.tokenizer\n",
    "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "        eval_dataset = TextDataset(tokenizer, args, args.test_data_file)\n",
    "\n",
    "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        for id, batch in enumerate(eval_dataloader):\n",
    "            input_ids, labels = batch\n",
    "            scores = self.explain(input_ids, labels)\n",
    "            print(scores)\n",
    "\n",
    "\n",
    "class Arg:\n",
    "    def __init__(self):\n",
    "        self.train_data_file = '../dataset/train.jsonl'\n",
    "        self.device = torch.device('cpu')\n",
    "        self.epoch = 5\n",
    "        self.train_batch_size = 16\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.learning_rate = 2e-5\n",
    "        self.max_grad_norm=1.0\n",
    "        self.weight_decay = 0.0\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.local_rank = -1\n",
    "        self.output_dir = '/Users/mahbubcseju/Desktop/projects/TransformerExplainability/saved_models/func_jsonal/64'\n",
    "        self.eval_data_file = '../dataset/valid.jsonl'\n",
    "        self.eval_batch_size = 1\n",
    "        self.evaluate_during_training = True\n",
    "        self.test_data_file = '/Users/mahbubcseju/Desktop/projects/TransformerExplainability/data/func_jsonal/test.jsonl'\n",
    "        self.idx_key='id'\n",
    "        self.block_size=400\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = Arg()\n",
    "\n",
    "    model = CodebertModel()\n",
    "    # model.train(args)\n",
    "    model.load_model(args)\n",
    "    model.explanation(args)\n",
    "    # model.test(args)\n",
    "    print(\"Sucess\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('665A')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76c1c3abec198d3a28acb00577a50bb14399c9cd08b83d8d0bc9d05da6e733fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
